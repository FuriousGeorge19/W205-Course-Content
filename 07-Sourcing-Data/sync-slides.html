<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Week 07 - sync session">
  <title>Fundamentals of Data Engineering</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="reveal.js/css/theme/mids.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Fundamentals of Data Engineering</h1>
  <h2 class="author">Week 07 - sync session</h2>
  <img class="frontPageSlogan" src="http://people.ischool.berkeley.edu/~mark.mims/course-development/2017-mids-w205/media/datascience-at-berkeley.png"/>
</section>

<section id="section" class="slide level1">
<h1></h1>
<section id="while-were-getting-started" class="level2">
<h2>While we’re getting started</h2>
<ul>
<li>Mid-Course Survey</li>
</ul>
<p>Please get course-eval links from slack</p>
</section>
<section id="assignment-06-breakout" class="level2">
<h2>Assignment 06 Breakout</h2>
</section>
</section>
<section id="section-1" class="slide level1">
<h1></h1>
<section id="section-2" class="level2">
<h2></h2>
<p><img data-src="images/streaming-bare.svg" style="border:0;box-shadow:none" /></p>
<aside class="notes">
<ul>
<li>Last week we published and consumed messages with kafka</li>
<li>Now we’ll consume messages with Spark and take a look at them</li>
<li>Next week, we’ll transform them in spark so we can land them in hdfs</li>
</ul>
</aside>
</section>
</section>
<section id="section-3" class="slide level1">
<h1></h1>
<section id="spark-stack-with-kafka" class="level2">
<h2>Spark Stack with Kafka</h2>
</section>
<section id="setup" class="level2">
<h2>Setup</h2>
<pre><code>mkdir ~/w205/spark-with-kafka
cd ~/w205/spark-with-kafka
cp ../course-content/07-Sourcing-Data/docker-compose.yml .</code></pre>
</section>
<section id="docker-compose.yml" class="level2">
<h2><code>docker-compose.yml</code></h2>
<pre><code>---
version: &#39;2&#39;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    expose:
      - &quot;2181&quot;
      - &quot;2888&quot;
      - &quot;32181&quot;
      - &quot;3888&quot;

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ~/w205:/w205
    expose:
      - &quot;9092&quot;
      - &quot;29092&quot;

  spark:
    image: midsw205/spark-python:0.0.5
    stdin_open: true
    tty: true
    volumes:
      - ~/w205:/w205
    command: bash

  mids:
    image: midsw205/base:latest
    stdin_open: true
    tty: true
    volumes:
      - ~/w205:/w205</code></pre>
<aside class="notes">
<p>and save as data file.</p>
</aside>
</section>
<section id="spin-up-the-cluster" class="level2">
<h2>Spin up the cluster</h2>
<pre><code>docker-compose up -d</code></pre>
<pre><code>docker-compose logs -f kafka</code></pre>
<aside class="notes">
<p>Now spin up the cluster</p>
<pre><code>docker-compose up -d</code></pre>
<p>and watch it come up</p>
<pre><code>    docker-compose logs -f kafka</code></pre>
<p>when this looks like it’s done, you can safely detach with <code>Ctrl-C</code>.</p>
</aside>
</section>
<section id="create-a-topic" class="level2">
<h2>create a topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic foo \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<p>First, create a topic <code>foo</code></p>
<pre><code>docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181</code></pre>
</aside>
</section>
<section id="should-show" class="level2">
<h2>Should show</h2>
<pre><code>Created topic &quot;foo&quot;.</code></pre>
</section>
<section id="check-the-topic" class="level2">
<h2>Check the topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
  --describe \
  --topic foo \
  --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<pre><code>docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181</code></pre>
</aside>
</section>
<section id="should-show-1" class="level2">
<h2>Should show</h2>
<pre><code>Topic:foo   PartitionCount:1    ReplicationFactor:1 Configs:
Topic: foo  Partition: 0    Leader: 1    Replicas: 1  Isr: 1</code></pre>
</section>
<section id="publish-some-stuff-to-kafka" class="level2">
<h2>Publish some stuff to kafka</h2>
<pre><code>docker-compose exec kafka \
  bash -c &quot;seq 42 | kafka-console-producer \
    --request-required-acks 1 \
    --broker-list kafka:29092 \
    --topic foo &amp;&amp; echo &#39;Produced 42 messages.&#39;&quot;</code></pre>
<aside class="notes">
<p>Use the kafka console producer to publish some test messages to that topic</p>
<pre><code>docker-compose exec kafka bash -c &quot;seq 42 | kafka-console-producer --request-required-acks 1 --broker-list kafka:29092 --topic foo &amp;&amp; echo &#39;Produced 42 messages.&#39;&quot;</code></pre>
</aside>
</section>
<section id="should-show-2" class="level2">
<h2>Should show</h2>
<pre><code>Produced 42 messages.</code></pre>
</section>
</section>
<section id="section-4" class="slide level1">
<h1></h1>
<section id="run-spark-using-the-spark-container" class="level2">
<h2>Run spark using the <code>spark</code> container</h2>
<pre><code>docker-compose exec spark pyspark</code></pre>
<aside class="notes">
<p>Spin up a pyspark process using the <code>spark</code> container</p>
<p>docker-compose exec spark pyspark</p>
<p>We have to add some kafka library dependencies on the cli for now.</p>
</aside>
</section>
<section id="read-stuff-from-kafka" class="level2">
<h2>read stuff from kafka</h2>
<p>At the pyspark prompt,</p>
<pre><code>numbers = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<p>At the pyspark prompt,</p>
<p>read from kafka</p>
<pre><code>numbers = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
</aside>
</section>
<section id="see-the-schema" class="level2">
<h2>See the schema</h2>
<pre><code>numbers.printSchema()</code></pre>
</section>
<section id="cast-it-as-strings" class="level2">
<h2>Cast it as strings</h2>
<pre><code>numbers_as_strings=numbers.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
<aside class="notes">
<p>cast it as strings (you can totally use <code>INT</code>s if you’d like)</p>
<pre><code>numbers_as_strings=numbers.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
</aside>
</section>
<section id="take-a-look" class="level2">
<h2>Take a look</h2>
<pre><code>numbers_as_strings.show()</code></pre>
<pre><code>numbers_as_strings.printSchema()</code></pre>
<pre><code>numbers_as_strings.count()</code></pre>
<aside class="notes">
<p>then you can exit pyspark using either <code>ctrl-d</code> or <code>exit()</code>.</p>
<pre><code>numbers_as_strings.show()
numbers_as_strings.printSchema()</code></pre>
</aside>
</section>
<section id="down" class="level2">
<h2>down</h2>
<pre><code>docker-compose down</code></pre>
</section>
</section>
<section id="section-5" class="slide level1">
<h1></h1>
<section id="spark-stack-with-kafka-with-real-messages" class="level2">
<h2>Spark stack with Kafka with “real” messages</h2>
<aside class="notes">

</aside>
</section>
<section id="docker-compose.yml-file" class="level2">
<h2>docker-compose.yml file</h2>
<ul>
<li>same</li>
<li>still in your <code>~/w205/spark-with-kafka</code></li>
</ul>
<aside class="notes">
<ul>
<li>same <code>docker-compose.yml</code> from above</li>
</ul>
</aside>
</section>
<section id="pull-data" class="level2">
<h2>Pull data</h2>
<pre><code>curl -L -o github-example-large.json https://goo.gl/Y4MD58</code></pre>
<aside class="notes">
<p>or copy (the same dataset) over from last week</p>
</aside>
</section>
<section id="spin-up-the-cluster-check" class="level2">
<h2>Spin up the cluster &amp; check</h2>
<pre><code>docker-compose up -d</code></pre>
<pre><code>docker-compose logs -f kafka</code></pre>
<aside class="notes">
<p>when this looks like it’s done, detach with <code>Ctrl-C</code></p>
</aside>
</section>
<section id="create-a-topic-1" class="level2">
<h2>create a topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --create \
      --topic foo \
      --partitions 1 \
      --replication-factor 1 \
      --if-not-exists \
      --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<pre><code>docker-compose exec kafka kafka-topics --create --topic foo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181</code></pre>
</aside>
</section>
<section id="should-see-something-like" class="level2">
<h2>Should see something like</h2>
<pre><code>Created topic &quot;foo&quot;.</code></pre>
</section>
<section id="check-the-topic-1" class="level2">
<h2>Check the topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --describe \
    --topic foo \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<pre><code>docker-compose exec kafka kafka-topics --describe --topic foo --zookeeper zookeeper:32181</code></pre>
</aside>
</section>
<section id="should-see-something-like-1" class="level2">
<h2>Should see something like</h2>
<pre><code>Topic:foo   PartitionCount:1    ReplicationFactor:1 Configs:
Topic: foo  Partition: 0    Leader: 1    Replicas: 1  Isr: 1</code></pre>
</section>
</section>
<section id="section-6" class="slide level1">
<h1></h1>
<section id="publish-real-data-to-kafka" class="level2">
<h2>Publish real data to kafka</h2>
</section>
<section id="check-out-our-messages" class="level2">
<h2>Check out our messages</h2>
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json&quot;
docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json | jq &#39;.&#39;&quot;</code></pre>
<aside class="notes">
<p>ugly 1st pretty print</p>
</aside>
</section>
<section id="individual-messages" class="level2">
<h2>Individual messages</h2>
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json | jq &#39;.[]&#39; -c&quot;</code></pre>
<aside class="notes">
<p>Go over | jq stuff</p>
<p>take time to go over jsonlines (Show examples from jsonlines.org)</p>
</aside>
</section>
<section id="publish-some-test-messages-to-that-topic-with-kafkacat" class="level2">
<h2>Publish some test messages to that topic with kafkacat</h2>
<pre><code>docker-compose exec mids \
  bash -c &quot;cat /w205/github-example-large.json \
    | jq &#39;.[]&#39; -c \
    | kafkacat -P -b kafka:29092 -t foo &amp;&amp; echo &#39;Produced 100 messages.&#39;&quot;</code></pre>
<aside class="notes">
<pre><code>     docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json | jq &#39;.[]&#39; -c | kafkacat -P -b kafka:29092 -t foo &amp;&amp; echo &#39;Produced 100 messages.&#39;&quot;</code></pre>
</aside>
</section>
<section id="should-see-something-like-2" class="level2">
<h2>Should see something like</h2>
<pre><code>Produced 100 messages.</code></pre>
</section>
</section>
<section id="section-7" class="slide level1">
<h1></h1>
<section id="run-spark-using-the-spark-container-1" class="level2">
<h2>Run spark using the <code>spark</code> container</h2>
<pre><code>docker-compose exec spark pyspark</code></pre>
<aside class="notes">
<p>Spin up a pyspark process using the <code>spark</code> container</p>
<p>docker-compose exec spark pyspark</p>
<p>We have to add some kafka library dependencies on the cli for now.</p>
</aside>
</section>
<section id="read-stuff-from-kafka-1" class="level2">
<h2>read stuff from kafka</h2>
<p>At the pyspark prompt,</p>
<pre><code>messages = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<p>At the pyspark prompt,</p>
<p>read from kafka</p>
<pre><code>messages = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
</aside>
</section>
<section id="see-the-schema-1" class="level2">
<h2>See the schema</h2>
<pre><code>messages.printSchema()</code></pre>
</section>
<section id="see-the-messages" class="level2">
<h2>See the messages</h2>
<pre><code>messages.show()</code></pre>
</section>
<section id="cast-as-strings" class="level2">
<h2>Cast as strings</h2>
<pre><code>messages_as_strings=messages.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
<aside class="notes">
<p>cast it as strings (you can totally use <code>INT</code>s if you’d like)</p>
<pre><code>messages_as_strings=messages.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
</aside>
</section>
<section id="take-a-look-1" class="level2">
<h2>Take a look</h2>
<pre><code>messages_as_strings.show()</code></pre>
<pre><code>messages_as_strings.printSchema()</code></pre>
<pre><code>messages_as_strings.count()</code></pre>
<aside class="notes">
<p>then you can exit pyspark using either <code>ctrl-d</code> or <code>exit()</code>.</p>
<pre><code>messages_as_strings.show()
messages_as_strings.printSchema()
messages_as_strings.count()</code></pre>
</aside>
</section>
<section id="unrolling-json" class="level2">
<h2>Unrolling json</h2>
<pre><code>messages_as_strings.select(&#39;value&#39;).take(1)</code></pre>
<pre><code>messages_as_strings.select(&#39;value&#39;).take(1)[0].value</code></pre>
<pre><code>import json</code></pre>
<pre><code>first_message=json.loads(messages_as_strings.select(&#39;value&#39;).take(1)[0].value)</code></pre>
<pre><code>first_message</code></pre>
<pre><code>print(first_message[&#39;commit&#39;][&#39;committer&#39;][&#39;name&#39;])</code></pre>
<aside class="notes">
<pre><code>messages_as_strings.select(&#39;value&#39;).take(1)
messages_as_strings.select(&#39;value&#39;).take(1)[0].value
&gt;&gt;&gt; import json
&gt;&gt;&gt; first_message=json.loads(messages_as_strings.select(&#39;value&#39;).take(1)[0].value)
&gt;&gt;&gt; first_message
&gt;&gt;&gt; print(first_message[&#39;commit&#39;][&#39;committer&#39;][&#39;name&#39;])
Nico Williams</code></pre>
</aside>
</section>
<section id="breakout" class="level2">
<h2>Breakout</h2>
<ul>
<li>Change around some of the fields to print different aspects of the commit</li>
</ul>
</section>
<section id="down-1" class="level2">
<h2>Down</h2>
<pre><code>docker-compose down</code></pre>
</section>
</section>
<section id="section-8" class="slide level1">
<h1></h1>
<section id="assignment-7" class="level2">
<h2>Assignment 7</h2>
<ul>
<li>Use Project 2 data</li>
<li>Step through this process (use spark to read from kafka)</li>
<li>What you turn in:</li>
<li>In your <code>/assignment-07-&lt;user-name&gt;</code> repo:
<ul>
<li>your <code>docker-compose.yml</code></li>
<li>once you’ve run the example on your terminal
<ul>
<li>Run <code>history &gt; &lt;user-name&gt;-history.txt</code></li>
<li>Save the relevant portion of your history as <code>&lt;user-name&gt;-annotations.md</code></li>
<li>Annotate the file with explanations of what you were doing at each point.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="week-7-videos" class="level2">
<h2>Week 7 Videos:</h2>
<ul>
<li>Sourcing Data</li>
<li>Context for Project 2 : The idea is where do you get the data that flows into our pipeline? You wouldn’t usually get it from a file.</li>
<li>Big Question: What do I need to do to data coming in to get it into Kafka? change the API client code? change app server code?</li>
</ul>
</section>
</section>
<section id="section-9" class="slide level1">
<h1></h1>
<section id="summary" class="level2">
<h2>Summary</h2>
<p><img data-src="images/streaming-bare.svg" style="border:0;box-shadow:none" /></p>
<aside class="notes">
<ul>
<li>Review what we just did</li>
</ul>
</aside>
</section>
</section>
<section id="section-10" class="slide level1">
<h1></h1>
<p><img class="logo" src="images/berkeley-school-of-information-logo.png"/></p>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Transition style
        transition: 'linear', // none/fade/slide/convex/concave/zoom

        math: {
          config: 'TeX-AMS_HTML-full'
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
      });
    </script>
    </body>
</html>
