<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Week 08 - sync session">
  <title>Fundamentals of Data Engineering</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="reveal.js/css/theme/mids.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Fundamentals of Data Engineering</h1>
  <h2 class="author">Week 08 - sync session</h2>
  <img class="frontPageSlogan" src="http://people.ischool.berkeley.edu/~mark.mims/course-development/2017-mids-w205/media/datascience-at-berkeley.png"/>
</section>

<section id="section" class="slide level1">
<h1></h1>
<section id="assignment-review" class="level2">
<h2>Assignment Review</h2>
<ul>
<li>Review your Assignment 07</li>
<li>Get ready to share</li>
</ul>
<aside class="notes">
<p>Breakout at about 5 after the hour: - Check in with each group - have students share screen</p>
</aside>
</section>
</section>
<section id="section-1" class="slide level1">
<h1></h1>
<section id="section-2" class="level2" data-background="images/streaming-bare.svg">
<h2></h2>
</section>
<section id="section-3" class="level2" data-background="images/streaming-bare-logos.jpg">
<h2></h2>
<aside class="notes">
<ul>
<li>Last week we consume messages with Spark and take a look at them</li>
<li>Now, we’ll transform them in spark so we can land them in hdfs</li>
<li>added cloudera but that’s just a distribution of hadoop</li>
</ul>
</aside>
</section>
</section>
<section id="section-4" class="slide level1">
<h1></h1>
<section id="spark-stack-with-kafka-and-hdfs" class="level2">
<h2>Spark Stack with Kafka and HDFS</h2>
</section>
<section id="setup" class="level2">
<h2>Setup</h2>
<pre><code>mkdir ~/w205/spark-with-kafka-and-hdfs</code></pre>
<pre><code>cd ~/w205/spark-with-kafka-and-hdfs</code></pre>
<pre><code>cp ~/w205/course-content//08-Querying-Data/docker-compose.yml .</code></pre>
<aside class="notes">
<p>Walk through the docker-compose.yml file</p>
</aside>
</section>
<section id="download-the-dataset-for-world-cup-players" class="level2">
<h2>Download the dataset for World Cup players</h2>
<ul>
<li>In <code>~/w205/</code></li>
</ul>
<pre><code>curl -L -o players.json https://goo.gl/vsuCpZ</code></pre>
<aside class="notes">
<p>easier than github dataset b/c it’s flat</p>
<p>really could be <code>~/w205/&lt;your_workspace&gt;</code> for this week!</p>
</aside>
</section>
<section id="spin-up-the-cluster" class="level2">
<h2>Spin up the cluster</h2>
<pre><code>docker-compose up -d</code></pre>
<pre><code>docker-compose logs -f kafka</code></pre>
<aside class="notes">
<p>Now spin up the cluster</p>
<pre><code>docker-compose up -d</code></pre>
<p>and watch it come up</p>
<pre><code>docker-compose logs -f kafka</code></pre>
<p>when this looks like it’s done, you can safely detach with <code>Ctrl-C</code>.</p>
</aside>
</section>
<section id="example-world-cup-players" class="level2">
<h2>Example: World Cup Players</h2>
</section>
<section id="check-out-hadoop" class="level2">
<h2>Check out Hadoop</h2>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
<aside class="notes">
<ul>
<li>look at tmp/ dir in hdfs and see that what we want to write isn’t there already</li>
<li>Can do ls options (h etc) to find out more</li>
</ul>
</aside>
</section>
<section id="should-see-something-like" class="level2">
<h2>Should see something like:</h2>
<pre><code>funwithflags:~/w205/spark-with-kafka-and-hdfs $ docker-compose exec cloudera hadoop fs -ls /tmp/
Found 2 items
drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn
drwx-wx-wx   - root   supergroup          0 2018-02-20 22:31 /tmp/hive</code></pre>
<aside class="notes">
<p>Let’s check out hdfs before we write anything to it</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
</aside>
</section>
<section id="create-a-topic-players" class="level2">
<h2>Create a topic <code>players</code></h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic players \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<p>First, create a topic <code>players</code></p>
<pre><code>docker-compose exec kafka kafka-topics --create --topic players --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181</code></pre>
<ul>
<li>Benefit, now don’t have to tear this down, we’ll have 2 different topics in the same kafka broker</li>
</ul>
</aside>
</section>
<section id="should-show" class="level2">
<h2>Should show</h2>
<pre><code>Created topic &quot;players&quot;.</code></pre>
</section>
<section id="use-kafkacat-to-produce-test-messages-to-the-players-topic" class="level2">
<h2>Use kafkacat to produce test messages to the <code>players</code> topic</h2>
<pre><code>docker-compose exec mids \
  bash -c &quot;cat /w205/&lt;your_workspace&gt;/players.json \
    | jq &#39;.[]&#39; -c \
    | kafkacat -P -b kafka:29092 -t players&quot;</code></pre>
<aside class="notes">
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/&lt;your_workspace&gt;/players.json | jq &#39;.[]&#39; -c | kafkacat -P -b kafka:29092 -t players&quot;</code></pre>
</aside>
</section>
</section>
<section id="section-5" class="slide level1">
<h1></h1>
<section id="spin-up-a-pyspark-process-using-the-spark-container" class="level2">
<h2>Spin up a pyspark process using the <code>spark</code> container</h2>
<pre><code>docker-compose exec spark pyspark</code></pre>
<aside class="notes">
<pre><code>docker-compose exec spark pyspark</code></pre>
<ul>
<li>Will move to ipython shell around week 9</li>
</ul>
</aside>
</section>
<section id="at-the-pyspark-prompt-read-from-kafka" class="level2">
<h2>At the pyspark prompt, read from kafka</h2>
<pre><code>raw_players = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;players&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<p>or, without the line-conitunations,</p>
<pre><code>raw_players = spark.read.format(&quot;kafka&quot;).option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;).option(&quot;subscribe&quot;,&quot;players&quot;).option(&quot;startingOffsets&quot;, &quot;earliest&quot;).option(&quot;endingOffsets&quot;, &quot;latest&quot;).load() </code></pre>
</aside>
</section>
<section id="cache-this-to-cut-back-on-warnings-later" class="level2">
<h2>Cache this to cut back on warnings later</h2>
<pre><code>raw_players.cache()</code></pre>
<aside class="notes">
<ul>
<li>Caching this to avoid warnings bc trying to hold a persistent handle to this topic.</li>
<li>Warning messages, e.g., you haven’t set your state to be good with kafka etc</li>
<li>What is caching?</li>
<li>Lazy evaluation
<ul>
<li>You won’t always get errors (it will be quiet) often until you do eval.</li>
</ul></li>
</ul>
</aside>
</section>
<section id="see-what-we-got" class="level2">
<h2>See what we got</h2>
<pre><code>raw_players.printSchema()</code></pre>
</section>
<section id="cast-it-as-strings-you-can-totally-use-ints-if-youd-like" class="level2">
<h2>Cast it as strings (you can totally use <code>INT</code>s if you’d like)</h2>
<pre><code>players = raw_players.select(raw_players.value.cast(&#39;string&#39;))</code></pre>
<p>or</p>
<pre><code>players = raw_players.selectExpr(&quot;CAST(value AS STRING)&quot;)</code></pre>
</section>
<section id="write-this-to-hdfs" class="level2">
<h2>Write this to hdfs</h2>
<pre><code>players.write.parquet(&quot;/tmp/players&quot;)</code></pre>
<aside class="notes">
<ul>
<li>this is writing it out to hadoop</li>
</ul>
</aside>
</section>
</section>
<section id="section-6" class="slide level1">
<h1></h1>
<section id="check-out-results-from-another-terminal-window" class="level2">
<h2>Check out results (from another terminal window)</h2>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
<p>and</p>
<pre><code>
docker-compose exec cloudera hadoop fs -ls /tmp/players/</code></pre>
<aside class="notes">
<p>You can see results in hadoop (from another terminal window)</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
<p>and</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/players/</code></pre>
<ul>
<li>Can do our h option for human readable</li>
</ul>
</aside>
</section>
<section id="however-back-in-spark-terminal-window" class="level2">
<h2>However (back in spark terminal window)</h2>
<ul>
<li>What did we actually write?</li>
</ul>
<pre><code>players.show()</code></pre>
<aside class="notes">
<ul>
<li>That’s pretty ugly… let’s extract the data, promote data cols to be real dataframe columns.</li>
<li>We have a single column which is a strign, our jsonlines,</li>
<li>can do the take 1, take players, get value etc</li>
<li>But that’s kinda lame</li>
<li>We want it to be easily queriable,</li>
<li>So, now let’s go in and unroll the json but let’s do it a dataframe at a time instead of a row at a time</li>
</ul>
</aside>
</section>
<section id="extract-data" class="level2">
<h2>Extract Data</h2>
</section>
<section id="deal-with-unicode" class="level2">
<h2>Deal with unicode</h2>
<pre><code>import sys
sys.stdout = open(sys.stdout.fileno(), mode=&#39;w&#39;, encoding=&#39;utf8&#39;, buffering=1)</code></pre>
<aside class="notes">
<ul>
<li>If a dataset gripes about parsing <code>ascii</code> characters, you might need to default to unicode…</li>
<li>it’s good practice in any case</li>
</ul>
</aside>
</section>
<section id="what-do-we-have" class="level2">
<h2>What do we have?</h2>
<ul>
<li>Take a look at</li>
</ul>
<pre><code>import json
players.rdd.map(lambda x: json.loads(x.value)).toDF().show()</code></pre>
<aside class="notes">
<ul>
<li>from the dataset, I’m going to get rdd (spark’s distributed dataset), apply a map to it (to work a df at a time)</li>
<li>taken a df, mapped it onto an rdd, ….., convert back to a spark dataframe and now we’re goig to show that</li>
<li>OK,so we have our unicode errors showing up, so go back to our unicode slide, and rerun</li>
<li>this is a df that looks like the content of our json and that’s nice</li>
<li>have to go through mapping bc you won;t have huge json file, you’ll have events that are json coming from kafka etc</li>
</ul>
</aside>
</section>
<section id="section-7" class="level2">
<h2></h2>
<pre><code>extracted_players = players.rdd.map(lambda x: json.loads(x.value)).toDF()</code></pre>
<pre><code>from pyspark.sql import Row
extracted_players = players.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()</code></pre>
<pre><code>extracted_players.show()</code></pre>
<aside class="notes">
<ul>
<li>Note that first one is deprecated. It’s easier to look at though.<br />
</li>
<li><p>The current recommended approach to this is to explicitly create our <code>Row</code> objects from the json fields (like in 2nd example)</p></li>
<li>Now we’re going to catch that and assign it to something.</li>
<li>This is just our player data.</li>
<li>** on top of dict will return key:value pairs, but that’s done in 1st row as well</li>
</ul>
</aside>
</section>
<section id="save-that" class="level2">
<h2>Save that</h2>
<pre><code>extracted_players.write.parquet(&quot;/tmp/extracted_players&quot;)</code></pre>
<aside class="notes">
<ul>
<li>This will be much easier to query.</li>
<li>So, goal in landing this into storage is to make it efficiently querieable</li>
<li>the first one wasn’t bc nested</li>
<li>but this is nice parquet that’s flat and has nice columns</li>
</ul>
</aside>
</section>
<section id="do" class="level2">
<h2>Do</h2>
<pre><code>players.show()</code></pre>
<pre><code>extracted_players.show()</code></pre>
<aside class="notes">
<ul>
<li>Compare these 2 based on notes in last slide.</li>
<li>This is great, but json is rarely flat,</li>
<li>So how do wee do all this with nested dataset?</li>
<li>We’ll have to make some choices.</li>
<li>Now we’re going to work through the whole dataframe (that’s what is different)</li>
</ul>
</aside>
</section>
</section>
<section id="section-8" class="slide level1">
<h1></h1>
<section id="example-github-commits" class="level2">
<h2>Example: GitHub Commits</h2>
<aside class="notes">
<ul>
<li>Note that we didn’t have to bring the cluster down bc we have 2 topics on our kafka broker now.</li>
<li>I’m going to leave spark running and go over here to new container</li>
</ul>
</aside>
</section>
<section id="check-out-hadoop-1" class="level2">
<h2>check out hadoop</h2>
<p>Let’s check out hdfs before we write anything to it</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
</section>
<section id="create-a-topic" class="level2">
<h2>Create a topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic commits \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<ul>
<li>First, create a topic <code>commits</code></li>
<li>Now actually naming our topics (ie not foo)</li>
<li>Fine to have multiple topics</li>
</ul>
<pre><code>docker-compose exec kafka kafka-topics --create --topic commits --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181</code></pre>
<ul>
<li><p>which should show</p>
Created topic “commits”.</li>
</ul>
</aside>
</section>
<section id="download-the-dataset-for-github-commits" class="level2">
<h2>Download the dataset for github commits</h2>
<pre><code>curl -L -o github-example-large.json https://goo.gl/Y4MD58</code></pre>
</section>
<section id="publish-some-stuff-to-kafka" class="level2">
<h2>Publish some stuff to kafka</h2>
<pre><code>docker-compose exec mids \
  bash -c &quot;cat /w205/&lt;your_workspace&gt;/github-example-large.json \
    | jq &#39;.[]&#39; -c \
    | kafkacat -P -b kafka:29092 -t commits&quot;</code></pre>
<aside class="notes">
<p>Use kafkacat to produce test messages to the <code>commits</code> topic</p>
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/&lt;your_workspace&gt;/github-example-large.json | jq &#39;.[]&#39; -c | kafkacat -P -b kafka:29092 -t commits&quot;</code></pre>
</aside>
</section>
<section id="spin-up-a-pyspark-process-using-the-spark-container-1" class="level2">
<h2>Spin up a pyspark process using the <code>spark</code> container</h2>
<pre><code>docker-compose exec spark pyspark</code></pre>
</section>
<section id="read-stuff-from-kafka" class="level2">
<h2>Read stuff from kafka</h2>
<ul>
<li>At the pyspark prompt, read from kafka</li>
</ul>
<pre><code>raw_commits = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;commits&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<pre><code>raw_commits = spark.read.format(&quot;kafka&quot;).option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;).option(&quot;subscribe&quot;,&quot;commits&quot;).option(&quot;startingOffsets&quot;, &quot;earliest&quot;).option(&quot;endingOffsets&quot;, &quot;latest&quot;).load() </code></pre>
<ul>
<li>We’re selecting the value by name</li>
<li>befor we were casting, very sql like</li>
<li>syntax for this in pyspark has changed greatly over time, keep an eye out for that
<ul>
<li>show structured streaming programming guides, <span class="citation" data-cites="spark/apache.org">@spark/apache.org</span></li>
</ul></li>
<li>Show can choose code in python (instead of scala etc)</li>
</ul>
</aside>
</section>
<section id="cache-this-to-cut-back-on-warnings" class="level2">
<h2>Cache this to cut back on warnings</h2>
<pre><code>raw_commits.cache()</code></pre>
<aside class="notes">
<ul>
<li>Can try it w/o caching in class to show warnings we get</li>
</ul>
</aside>
</section>
<section id="see-what-we-got-1" class="level2">
<h2>See what we got</h2>
<pre><code>raw_commits.printSchema()</code></pre>
</section>
<section id="take-the-values-as-strings" class="level2">
<h2>Take the <code>value</code>s as strings</h2>
<pre><code>commits = raw_commits.select(raw_commits.value.cast(&#39;string&#39;))</code></pre>
</section>
<section id="of-course-we-could-just-write-this-to-hdfs" class="level2">
<h2>Of course, we <em>could</em> just write this to hdfs</h2>
<pre><code>commits.write.parquet(&quot;/tmp/commits&quot;)</code></pre>
<ul>
<li>but let’s extract the data a bit first…</li>
</ul>
</section>
<section id="extract-more-fields" class="level2">
<h2>Extract more fields</h2>
<ul>
<li>Let’s extract our json fields again</li>
</ul>
<pre><code>extracted_commits = commits.rdd.map(lambda x: json.loads(x.value)).toDF()</code></pre>
</section>
<section id="and-see" class="level2">
<h2>and see</h2>
<pre><code>extracted_commits.show()</code></pre>
<aside class="notes">
<ul>
<li>We see it’s useless because of the nesting.</li>
</ul>
</aside>
</section>
<section id="hmmm-did-all-of-our-stuff-get-extracted" class="level2">
<h2>hmmm… did all of our stuff get extracted?</h2>
<pre><code>extracted_commits.printSchema()</code></pre>
<ul>
<li>Problem: more nested json than before</li>
</ul>
<aside class="notes">
<ul>
<li>what’s going on?</li>
<li>The problem is more nested json than before.</li>
<li>Point of the project. Deal with this nested array. Which might change the cardinality of the dataset. (Discuss what that means)</li>
<li>Here’s a nice way to deal with that.</li>
</ul>
</aside>
</section>
<section id="use-sparksql" class="level2">
<h2>Use SparkSQL</h2>
<ul>
<li>First, create a Spark “TempTable” (aka “View”)</li>
</ul>
<pre><code>extracted_commits.registerTempTable(&#39;commits&#39;)</code></pre>
<aside class="notes">
<ul>
<li><p>We’ll use <code>SparkSQL</code> to let us easily pick and choose the fields we want to promote to columns.</p></li>
<li>Note that there are other ways to extract nested data, but <code>SparkSQL</code> is the easiest way to see what’s going on.</li>
<li>Now we can start doing more freeform sql</li>
</ul>
</aside>
</section>
<section id="then-we-can-create-dataframes-from-queries" class="level2">
<h2>Then we can create DataFrames from queries</h2>
<pre><code>spark.sql(&quot;select commit.committer.name from commits limit 10&quot;).show()</code></pre>
<pre><code>spark.sql(&quot;select commit.committer.name, commit.committer.date, sha from commits limit 10&quot;).show()</code></pre>
<aside class="notes">
<ul>
<li>1st just mines commits</li>
<li>View what you’d really like to see</li>
<li>You’re transforming the data in subh a way that you’re throwing things away.</li>
<li>When we read from kafka with spark did the data leave kafka (ie could we just rerun it all)</li>
<li>Where spark sql can really benefit you in your job.</li>
</ul>
</aside>
</section>
<section id="grab-what-we-want" class="level2">
<h2>Grab what we want</h2>
<pre><code>some_commit_info = spark.sql(&quot;select commit.committer.name, commit.committer.date, sha from commits limit 10&quot;)</code></pre>
<aside class="notes">
<ul>
<li>Compare to last week looking at this stuff using jq</li>
</ul>
</aside>
</section>
<section id="write-to-hdfs" class="level2">
<h2>Write to hdfs</h2>
<ul>
<li>We can write that out</li>
</ul>
<pre><code>some_commit_info.write.parquet(&quot;/tmp/some_commit_info&quot;)</code></pre>
</section>
<section id="check-out-results" class="level2">
<h2>Check out results</h2>
<p>-You can see results in hadoop</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/</code></pre>
<p>and</p>
<pre><code>docker-compose exec cloudera hadoop fs -ls /tmp/commits/</code></pre>
</section>
<section id="exit" class="level2">
<h2>Exit</h2>
<ul>
<li>Remember, you can exit pyspark using either <code>ctrl-d</code> or <code>exit()</code>.</li>
</ul>
</section>
<section id="down" class="level2">
<h2>Down</h2>
<pre><code>docker-compose down</code></pre>
</section>
</section>
<section id="section-9" class="slide level1">
<h1></h1>
<section id="summary" class="level2">
<h2>Summary</h2>
</section>
<section id="section-10" class="level2" data-background="images/streaming-bare-logos.jpg">
<h2></h2>
<aside class="notes">
<ul>
<li>What did we do in this pipeline this week</li>
<li>Persistence of data in our pipeline</li>
<li>We can set the retention in kafka</li>
<li>currently we’re using it as a buffer so keeping it for like an hour</li>
<li>say we chose to do the summary, we threw a lot of the data away</li>
<li>say I land that into hadoop to store long term</li>
<li>this is super clean for querying, but I’ve lost the data</li>
<li>so a lot of people leave the data in kakfa as well so they can go back and check it again</li>
<li>kafka has the same kind of replicaton factors and partitions as hadoop</li>
<li>gives you the choice</li>
</ul>
</aside>
</section>
<section id="week-8-videos" class="level2">
<h2>Week 8 Videos</h2>
<ul>
<li>Context for similar pipeline in reality( aka, at scale, in production)</li>
<li>Presto query walkthrough</li>
<li>Partitioning</li>
<li>How query jobs are scheduled &amp; executed with distributed resources</li>
</ul>
</section>
</section>
<section id="section-11" class="slide level1">
<h1></h1>
<p><img class="logo" src="images/berkeley-school-of-information-logo.png"/></p>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Transition style
        transition: 'linear', // none/fade/slide/convex/concave/zoom

        math: {
          config: 'TeX-AMS_HTML-full'
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
      });
    </script>
    </body>
</html>
